---
import Layout from '../../../layouts/Layout.astro'
import Nav from '../../../components/Nav.astro'
import Code from '../../../components/Code.astro'
---
<Layout title="Use chomp as an agent tool" description="Give any AI agent access to 50+ free models with a single API call." path="/docs/guides/tool" type="article" keywords="chomp, agent tool, AI agent, free models, API, LLM proxy, OpenAI SDK, dispatch">
  <Nav active="/docs/guides" />
  <main class="max-w-3xl mx-auto px-5 py-12">
    <div class="text-sm font-semibold text-green-500 mb-2">How-to guide</div>
    <h1 class="text-3xl font-bold tracking-tight mb-2">Use chomp as an agent tool</h1>
    <p class="text-zinc-500 dark:text-zinc-400 mb-10">Give any AI agent access to 50+ free models with a single API call.</p>
    <section class="space-y-12">
      <div>
        <h2 class="text-xl font-bold mb-3">The pattern</h2>
        <p class="text-zinc-600 dark:text-zinc-400 mb-3">Your orchestrating agent (Claude, GPT-4, etc.) can offload subtasks to free models to save tokens and money. Instead of using its own expensive context for summarization, formatting, or classification, it delegates to chomp.</p>
      </div>
      <div>
        <h2 class="text-xl font-bold mb-3">Shell tool (simplest)</h2>
        <p class="text-zinc-600 dark:text-zinc-400 mb-3">If your agent can run shell commands:</p>
        <Code code={`# Dispatch and wait for result
JOB=$(curl -s -X POST https://chomp.coey.dev/api/dispatch \\
  -H "Authorization: Bearer $CHOMP_TOKEN" \\
  -H "Content-Type: application/json" \\
  -d '{"prompt": "Summarize this in 3 bullets: ..."}')
JOB_ID=$(echo $JOB | jq -r .id)

# Poll until done
while true; do
  RESULT=$(curl -s https://chomp.coey.dev/api/result/$JOB_ID \\
    -H "Authorization: Bearer $CHOMP_TOKEN")
  STATUS=$(echo $RESULT | jq -r .status)
  [ "$STATUS" != "running" ] && break
  sleep 2
done
echo $RESULT | jq -r .result`} />
      </div>
      <div>
        <h2 class="text-xl font-bold mb-3">OpenAI SDK (drop-in)</h2>
        <p class="text-zinc-600 dark:text-zinc-400 mb-3">Point any OpenAI-compatible client at chomp's local proxy:</p>
        <Code code={`from openai import OpenAI

# Point at local chomp proxy
client = OpenAI(
    base_url="http://localhost:8001/v1",
    api_key="your-chomp-token",
)

response = client.chat.completions.create(
    model="auto",  # chomp picks the best free model
    messages=[
        {"role": "system", "content": "Summarize in 3 bullet points."},
        {"role": "user", "content": file_contents},
    ],
)
print(response.choices[0].message.content)`} />
      </div>
      <div>
        <h2 class="text-xl font-bold mb-3">TypeScript/JavaScript</h2>
        <p class="text-zinc-600 dark:text-zinc-400 mb-3">Using fetch directly:</p>
        <Code code={`const response = await fetch("https://chomp.coey.dev/api/dispatch", {
  method: "POST",
  headers: {
    "Authorization": \`Bearer \${process.env.CHOMP_TOKEN}\`,
    "Content-Type": "application/json",
  },
  body: JSON.stringify({
    prompt: "Classify this text as positive, negative, or neutral: ...",
    system: "Reply with only one word: positive, negative, or neutral.",
  }),
})
const { id } = await response.json()
// Poll /api/result/\${id} until status !== "running"`} />
      </div>
      <div>
        <h2 class="text-xl font-bold mb-3">When to use which</h2>
        <ul class="list-disc pl-6 space-y-2 mb-3 text-zinc-600 dark:text-zinc-400">
          <li>Local proxy (<code class="bg-zinc-200 dark:bg-zinc-800 px-1.5 py-0.5 rounded text-sm">/v1/chat/completions</code>) for synchronous, low-latency calls.</li>
          <li>Dispatch API (<code class="bg-zinc-200 dark:bg-zinc-800 px-1.5 py-0.5 rounded text-sm">/api/dispatch</code> + <code class="bg-zinc-200 dark:bg-zinc-800 px-1.5 py-0.5 rounded text-sm">/api/result</code>) for async, long-running tasks.</li>
          <li>MCP tools for Claude Desktop, Cursor, and other MCP clients.</li>
        </ul>
      </div>
    </section>
  </main>
</Layout>
